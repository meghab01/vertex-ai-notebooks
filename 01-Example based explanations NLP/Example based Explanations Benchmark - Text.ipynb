{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to get example-based explanations for your natural language model. Such explanations can help you answer questions about why a model made a certain prediction, relating it to the points in the training data. \n",
    "\n",
    "The pre-requisities for this notebook are:\n",
    "1. A predictive model and a way to extract a latent representation from it (usually termed embeddings). The notebook will demonstrate how this can be done for a Deep Neural Network.\n",
    "2. A Google Cloud project.\n",
    "3. A Google bucket to host the model and the dataset.\n",
    "\n",
    "Once these are in place, the three main sections of the notebook are:\n",
    "1. Creating and uploading a model with explanations enabled.\n",
    "2. Creating an endpoint and deploying the model to it.\n",
    "3. Issuing explanation request and inspecting them.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "For this notebook, we will use the [ag_news_subset] downloaded through [TF Datasets](https://www.tensorflow.org/datasets/catalog/ag_news_subset). The AG's news topic classification dataset is constructed by choosing 4 largest classes from the original corpus\n",
    "\n",
    "### Objective\n",
    "\n",
    "In this demo we will go over:\n",
    "1. Getting Example-Based explanations from Vertex Explainable AI services.\n",
    "2. A use-case for exploring similar examples to understand model predictions.\n",
    "\n",
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or Google Cloud Notebooks, you can skip this step**, since your environment already meets\n",
    "all the requirements to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip3 install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### Install and upgrade needed packages\n",
    "\n",
    "No additional packages are needed for this notebook, and we upgrade each relevant package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bHb-DP1ieozQ"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"exp-mb\"\n",
    "BUCKET_URI = \"gs://demo-central1\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Run the following cell to create your Cloud Storage bucket.** If the bucket already exists, you will get an error but it wouldn't affect the rest of the tutorial. However, you might get unwanted data in this bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import io\n",
    "import base64\n",
    "\n",
    "# Training\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from PIL import Image\n",
    "\n",
    "# Vertex AI\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import aiplatform_v1beta1 as vertex_ai_v1beta1\n",
    "from google.cloud.aiplatform_v1beta1 import ModelServiceClient, EndpointServiceClient\n",
    "from google.cloud.aiplatform_v1beta1.types import io as io_pb2\n",
    "from google.protobuf.struct_pb2 import Struct\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Set up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-P3O55RyMGtC"
   },
   "outputs": [],
   "source": [
    "# Client variables\n",
    "DATASET_NAME = \"ag_news_subset\" # Will be downloaded from https://www.tensorflow.org/datasets/catalog/ag_news_subset\n",
    "ENDPOINT=f\"{REGION}-aiplatform.googleapis.com\"\n",
    "ADMIN_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\"\n",
    "\n",
    "# Data preparation\n",
    "CONCRETE_INPUT = 'numpy_inputs' # input name needed for model signature\n",
    "DATA_DIR = 'data'\n",
    "RAW_DIR = f'{DATA_DIR}/raw/{DATASET_NAME}'\n",
    "PREPROCESSED_DIR = f'{DATA_DIR}/preprocessed/{DATASET_NAME}'\n",
    "\n",
    "# Saved model path\n",
    "PRETRAINED_MODEL_NAME = \"nnlm-en-dim50\"\n",
    "MODEL_URI = f'{BUCKET_URI}/models/{PRETRAINED_MODEL_NAME}-{DATASET_NAME}'  # Pre-trained model will be downloaded from https://keras.io/api/applications/mobilenet/#mobilenetv2-function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMY97zSZLVj7"
   },
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tuhbXWKrLYuk"
   },
   "outputs": [],
   "source": [
    "def create_index_to_name_map(ds_info):\n",
    "    index_to_name = {}\n",
    "    num_classes = ds_info.features[\"label\"].num_classes\n",
    "    names = ds_info.features[\"label\"].names\n",
    "    for i in range(num_classes):\n",
    "        index_to_name[i] = names[i]\n",
    "    return index_to_name\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec([None], tf.string), tf.TensorSpec(shape=[None], dtype=tf.string)])\n",
    "def serving_fn(id, input_text):\n",
    "    embedding = embedding_model(input_text)\n",
    "    return {\"id\": id, \"embedding\": embedding}\n",
    "\n",
    "\n",
    "def extract_examples_and_labels(ds, num_batches):\n",
    "    data_slice = ds.take(num_batches)  # -1 uses the whole dataset\n",
    "    print(num_batches)\n",
    "    examples = []\n",
    "    labels = []\n",
    "    for example, label in data_slice:\n",
    "        examples.append(example)\n",
    "        labels.append(label)\n",
    "    print(f\"Example batch shape: {len(examples)}\")\n",
    "    examples = tf.concat(examples,0)\n",
    "    labels = tf.concat(labels,0)\n",
    "    print(f\"Example batch shape: {examples.shape}\")\n",
    "    return examples.numpy(), labels.numpy()\n",
    "\n",
    "def inspect_input_and_neighbors(val_example_idx, all_train_examples, val_examples, all_train_labels, val_labels, label_index_to_name, data_with_neighbors):\n",
    "    TEXT_WIDTH = 200\n",
    "    example = val_examples[val_example_idx].numpy()\n",
    "    class_label = val_labels[val_example_idx].numpy()\n",
    "    print(f\"Input label: {label_index_to_name[class_label]}({class_label}). Input index: {val_example_idx}. Input example:\\n{textwrap.fill(example.decode('utf-8'), width = TEXT_WIDTH)}\\n\")\n",
    "    \n",
    "    neighbor_list = data_with_neighbors[val_example_idx]['neighbors']\n",
    "    num_neighbors = len(neighbor_list)\n",
    "    for n in range(num_neighbors):\n",
    "        neighbor = neighbor_list[n]\n",
    "        neighbor_idx = int(neighbor['neighborId'])\n",
    "        neighbor_example = all_train_examples[neighbor_idx].numpy()\n",
    "        neighbor_dist = neighbor['neighborDistance']\n",
    "        \n",
    "        class_label = all_train_labels[neighbor_idx].numpy()\n",
    "        print(f\"Neighbor label: {label_index_to_name[class_label]}({class_label}). Neighbor index: {neighbor_idx}. \"\n",
    "            f\"Neighbor distance: {neighbor_dist:.3f}. Neighbor example:\\n {textwrap.fill(neighbor_example.decode('utf-8'), width = TEXT_WIDTH)}\\n\")\n",
    "    print(\"*****************\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAXCt-Ldplv2"
   },
   "source": [
    "### Intialize Vertex AI Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('job', <google.cloud.aiplatform_v1beta1.services.job_service.client.JobServiceClient object at 0x7fadafc65d90>)\n",
      "('model', <google.cloud.aiplatform_v1beta1.services.model_service.client.ModelServiceClient object at 0x7fadafc658d0>)\n",
      "('endpoint', <google.cloud.aiplatform_v1beta1.services.endpoint_service.client.EndpointServiceClient object at 0x7fad85871a50>)\n",
      "('prediction', <google.cloud.aiplatform_v1beta1.services.prediction_service.client.PredictionServiceClient object at 0x7fad857ba190>)\n"
     ]
    }
   ],
   "source": [
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "\n",
    "# client options same for all services\n",
    "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "\n",
    "\n",
    "def create_job_client():\n",
    "    client = vertex_ai_v1beta1.JobServiceClient(client_options=client_options)\n",
    "    return client\n",
    "\n",
    "\n",
    "def create_model_client():\n",
    "    client = vertex_ai_v1beta1.ModelServiceClient(client_options=client_options)\n",
    "    return client\n",
    "\n",
    "\n",
    "def create_endpoint_client():\n",
    "    client = vertex_ai_v1beta1.EndpointServiceClient(client_options=client_options)\n",
    "    return client\n",
    "\n",
    "\n",
    "def create_prediction_client():\n",
    "    client = vertex_ai_v1beta1.PredictionServiceClient(client_options=client_options)\n",
    "    return client\n",
    "\n",
    "\n",
    "clients = {}\n",
    "clients[\"job\"] = create_job_client()\n",
    "clients[\"model\"] = create_model_client()\n",
    "clients[\"endpoint\"] = create_endpoint_client()\n",
    "clients[\"prediction\"] = create_prediction_client()\n",
    "\n",
    "for client in clients.items():\n",
    "    print(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDKXS5h2LpNB"
   },
   "source": [
    "### Prepare Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDDCBHJbgW7n"
   },
   "source": [
    "#### Download and visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KGiCMhPeSYdy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-13 16:55:19.404382: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2022-10-13 16:55:19.578976: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-10-13 16:55:19.758683: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "tfds.disable_progress_bar()\n",
    "train_split = \"train\"\n",
    "test_split = \"test\"\n",
    "split_ds, ds_info = tfds.load(\n",
    "    DATASET_NAME,\n",
    "    split=[train_split, test_split],\n",
    "    as_supervised=True,  # Include labels\\\n",
    "    with_info=True,\n",
    "    shuffle_files=False,\n",
    "    data_dir=RAW_DIR\n",
    ")\n",
    "train_ds, validation_ds = split_ds\n",
    "tfds.as_dataframe(ds=train_ds.take(10), ds_info=ds_info);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sGNlBKlWju5t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes in the dataset: 4\n",
      "Label names: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
      "Number of examples in training split: 120000\n",
      "Number of examples in validation split: 7600\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of classes in the dataset: {ds_info.features[\"label\"].num_classes}')\n",
    "print(f'Label names: {ds_info.features[\"label\"].names}')\n",
    "# print(ds_info.features[\"label\"].int2str(7))  # Human readable version\n",
    "print(f'Number of examples in training split: {ds_info.splits[train_split].num_examples}')\n",
    "print(f'Number of examples in validation split: {ds_info.splits[test_split].num_examples}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjds2n3Bj-9w"
   },
   "source": [
    "#### Batch and prefetch data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "t1GjhJODkJRh"
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "train_ds = train_ds.batch(batch_size).prefetch(buffer_size=10)\n",
    "validation_ds = validation_ds.batch(batch_size).prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vlhPZStokO90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'World', 1: 'Sports', 2: 'Business', 3: 'Sci/Tech'}\n"
     ]
    }
   ],
   "source": [
    "label_index_to_name = create_index_to_name_map(ds_info)\n",
    "print(label_index_to_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a pre-trained embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lxnUTl_2i4oK"
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "pre_trained_embedding_model_loc = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "pre_trained_embedding_model = hub.KerasLayer(pre_trained_embedding_model_loc, input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for the batch: [[ 0.13279007  0.06140124  0.1747397  ...  0.00377896  0.1353013\n",
      "   0.35294548]\n",
      " [ 0.2955813   0.134404    0.09672645 ... -0.19247894 -0.08087391\n",
      "  -0.32990122]\n",
      " [ 0.46365982 -0.18259482  0.07452042 ...  0.07921226 -0.1383935\n",
      "   0.28922987]\n",
      " ...\n",
      " [ 0.11779197  0.22901958 -0.14160861 ... -0.11089166 -0.09499406\n",
      "  -0.13324186]\n",
      " [ 0.3542565  -0.3827607   0.22900873 ... -0.02005082  0.36086604\n",
      "  -0.09353664]\n",
      " [ 0.13835187 -0.07392867  0.0599376  ... -0.22547187  0.23192091\n",
      "   0.23756932]]\n",
      "\n",
      "\n",
      "Dataset shape: (512, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-13 16:55:33.018483: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Small test to inspect embeddings from the pre-trained model\n",
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  # print(text_batch.numpy()[:5])\n",
    "    embedding = pre_trained_embedding_model(text_batch)\n",
    "    print(f\"Embedding for the batch: {embedding}\\n\\n\")\n",
    "    print(f\"Dataset shape: {embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 50)                48190600  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                816       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 48,191,484\n",
      "Trainable params: 48,191,484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_classes = ds_info.features[\"label\"].num_classes\n",
    "model = tf.keras.Sequential()\n",
    "model.add(pre_trained_embedding_model)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(num_classes))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 82/235 [=========>....................] - ETA: 57s - loss: 0.9550 - sparse_categorical_accuracy: 0.6949"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.SparseCategoricalAccuracy()],)\n",
    "\n",
    "epochs = 3\n",
    "history = model.fit(train_ds, epochs=epochs, validation_data=validation_ds, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxzsSYunlFGh"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('Model Accuracy', fontsize=20)\n",
    "plt.ylabel('Accuracy', fontsize=15)\n",
    "plt.xlabel('Epoch', fontsize=15)\n",
    "plt.legend(['Train', 'Test'], fontsize=15)\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss', fontsize=20)\n",
    "plt.ylabel('Loss', fontsize=15)\n",
    "plt.xlabel('Epoch', fontsize=15)\n",
    "plt.legend(['Train', 'Test'], fontsize=15)\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "def display_confusion_matrix(true_labels, predicted_labels, label_index_to_name):\n",
    "    num_classes = len(label_index_to_name)\n",
    "    sorted_class_names = [f\"{i}:{label_index_to_name[i]}\" for i in range(num_classes)]\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    df_cm = pd.DataFrame(cm, index = sorted_class_names, columns = sorted_class_names)\n",
    "    plt.figure(figsize = (10,10))\n",
    "    sn.heatmap(df_cm, annot=True, cbar=False, square=True, fmt=\"d\")\n",
    "    plt.xlabel(\"Predicted Label\", fontsize=15)\n",
    "    plt.ylabel(\"True Label\", fontsize=15)\n",
    "    \n",
    "validation_true_labels = np.concatenate([y for _, y in validation_ds], axis=0)\n",
    "predicted_labels = np.argmax(model.predict(validation_ds), axis=1)\n",
    "display_confusion_matrix(validation_true_labels, predicted_labels, label_index_to_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings. Generate model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = keras.models.Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer('keras_layer').output)\n",
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = keras.Sequential([model, \n",
    "                                         tf.keras.layers.Softmax()])\n",
    "probability_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(embedding_model, MODEL_URI, signatures={\n",
    "    'serving_default': serving_fn,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4Y9WltYOwIE"
   },
   "source": [
    "## Upload the training data\n",
    "The explanations are picked from this data. You can choose a smaller number of batches for a faster run but the results would be less precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "KrKfZB_xw0Qu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "Example batch shape: 235\n",
      "Example batch shape: (120000,)\n",
      "Time taken to process training data: 5.32566 secs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.exists(PREPROCESSED_DIR):\n",
    "    os.makedirs(PREPROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "dataset_file = f'{DATASET_NAME}-train-examples.jsonl'\n",
    "saved_jsonl_path = f'{PREPROCESSED_DIR}/{dataset_file}'\n",
    "input_tensor_name = 'input_text'  # Must match the serving_fn definition\n",
    "\n",
    "num_batches = -1 # uses the entire dataset\n",
    "start = time.time()\n",
    "all_train_examples, all_train_labels = extract_examples_and_labels(train_ds, num_batches=num_batches)\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ALNS_gFnTWjS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to create the training data: 1.52595 secs\n",
      "\n",
      "Copying file://data/preprocessed/ag_news_subset/ag_news_subset-train-examples.jsonl [Content-Type=application/octet-stream]...\n",
      "\\ [1 files][ 26.5 MiB/ 26.5 MiB]                                                \n",
      "Operation completed over 1 objects/26.5 MiB.                                     \n",
      "Time taken to create and upload the training data: 3.27628 secs\n",
      "Time taken to upload the training data for 1 iterations: [3.2762773036956787]\n"
     ]
    }
   ],
   "source": [
    "def create_training_data(saved_jsonl_path, all_train_examples):\n",
    "    with open(saved_jsonl_path, 'w') as f:\n",
    "        for i,ex in enumerate(all_train_examples):\n",
    "            json.dump({'id':str(i), input_tensor_name:str(ex)}, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "def upload_training_data(saved_jsonl_path, BUCKET_URI):\n",
    "    ! gsutil cp {saved_jsonl_path} {BUCKET_URI}\n",
    "\n",
    "# def benchmark_upload_training_data(saved_jsonl_path, BUCKET_URI, NUM_ITERATIONS):\n",
    "#     upload_time_taken = []\n",
    "#     for i in range(NUM_ITERATIONS):\n",
    "#         start = time.time()\n",
    "#         upload_training_data(saved_jsonl_path, BUCKET_URI)\n",
    "#         end = time.time()\n",
    "#         time_taken = end - start\n",
    "#         upload_time_taken.append(time_taken)\n",
    "#         print(f\"Time taken to create and upload the training data: {time_taken:.5f} secs\")\n",
    "#     print(f\"Time taken to upload the training data for {NUM_ITERATIONS} iterations: {upload_time_taken}\")\n",
    "#     return upload_time_taken\n",
    "        \n",
    "# start = time.time()\n",
    "create_training_data(saved_jsonl_path, all_train_examples)\n",
    "upload_training_data(saved_jsonl_path, BUCKET_URI)\n",
    "# end = time.time()\n",
    "# print(f\"Time taken to create the training data: {end - start:.5f} secs\\n\")\n",
    "\n",
    "# upload_time_taken = benchmark_upload_training_data(saved_jsonl_path, BUCKET_URI, NUM_ITERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zscb1GkHqGwF"
   },
   "source": [
    "# Upload the model with the configuration\n",
    "\n",
    "This step can take up to an hour to finish. Currently, there is no easy way to monitor this progresses. Exposing the run logs and job status is planned for near future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnCl2QeT49Aq"
   },
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "On93CSnMTH1R"
   },
   "source": [
    "#### Set Model Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "GqsCX7fjlFYz"
   },
   "outputs": [],
   "source": [
    "DIMENSIONS = embedding_model.output.shape[1]\n",
    "DATASET_FILE_PATH = f'{BUCKET_URI}/{dataset_file}'\n",
    "DEPLOY_IMAGE_URI = \"gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-5:latest\"\n",
    "MODEL_NAME = f\"similarity-{DATASET_NAME}-{TIMESTAMP}\"\n",
    "PARENT = f'projects/{PROJECT_ID}/locations/{REGION}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dzHO0Q3TpNC"
   },
   "source": [
    "#### Define serving container configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "HFT87_s0ToZK"
   },
   "outputs": [],
   "source": [
    "CONTAINER_CONFIG = {\n",
    "    \"image_uri\": DEPLOY_IMAGE_URI\n",
    "}\n",
    "\n",
    "CONTAINER_SPEC = vertex_ai_v1beta1.types.ModelContainerSpec(CONTAINER_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDIU8t7UTLiv"
   },
   "source": [
    "#### Define Example-Based Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "3fvyCnjdA_xD"
   },
   "outputs": [],
   "source": [
    "NNS_CONFIG = {\n",
    "              \"contentsDeltaUri\": \"\", \n",
    "              \"config\": {\n",
    "                \"dimensions\": DIMENSIONS,\n",
    "                \"approximateNeighborsCount\": 10,\n",
    "                \"distanceMeasureType\": \"SQUARED_L2_DISTANCE\",\n",
    "                \"featureNormType\": \"NONE\",\n",
    "                \"algorithmConfig\": {\n",
    "                \"treeAhConfig\": {\n",
    "                    \"leafNodeEmbeddingCount\": 1000,\n",
    "                    \"leafNodesToSearchPercent\": 100\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "\n",
    "NUM_NEIGHBORS_TO_RETURN = 10\n",
    "\n",
    "examples = vertex_ai_v1beta1.Examples(nearest_neighbor_search_config=NNS_CONFIG, \n",
    "                                      gcs_source=io_pb2.GcsSource(uris=[DATASET_FILE_PATH]), \n",
    "                                      neighbor_count=NUM_NEIGHBORS_TO_RETURN)\n",
    "\n",
    "EXPLANATION_PARAMS_CONFIG = vertex_ai_v1beta1.types.ExplanationParameters(examples=examples)\n",
    "\n",
    "EXPLANATION_INPUTS = {\n",
    "    \n",
    "          \"my_input\": vertex_ai_v1beta1.types.ExplanationMetadata.InputMetadata(\n",
    "              {\n",
    "              \"input_tensor_name\": input_tensor_name,\n",
    "              \"encoding\": vertex_ai_v1beta1.types.ExplanationMetadata.InputMetadata.Encoding(1), #'IDENTITY'\n",
    "              \"modality\": \"image\"\n",
    "              }\n",
    "          ),\n",
    "          \"id\": vertex_ai_v1beta1.types.ExplanationMetadata.InputMetadata(\n",
    "              {\n",
    "              \"input_tensor_name\": \"id\",\n",
    "              \"encoding\": vertex_ai_v1beta1.types.ExplanationMetadata.InputMetadata.Encoding(1) #'IDENTITY'\n",
    "              }\n",
    "          )\n",
    "}\n",
    "\n",
    "EXPLANATION_OUTPUTS = {\n",
    "    \"embedding\" : vertex_ai_v1beta1.types.ExplanationMetadata.OutputMetadata(\n",
    "        {\n",
    "            \"output_tensor_name\": \"embedding\"\n",
    "        }\n",
    "        \n",
    "    )\n",
    "}\n",
    "\n",
    "EXPLANATION_META_CONFIG = vertex_ai_v1beta1.types.ExplanationMetadata(\n",
    "    inputs = EXPLANATION_INPUTS,\n",
    "    outputs = EXPLANATION_OUTPUTS\n",
    ")\n",
    "\n",
    "EXPLANATION_SPEC = vertex_ai_v1beta1.types.ExplanationSpec(\n",
    "    parameters=EXPLANATION_PARAMS_CONFIG,\n",
    "    metadata=EXPLANATION_META_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "xW8iqoqFTa5l"
   },
   "outputs": [],
   "source": [
    "MODEL_CONFIGURATION = {\n",
    "    \"display_name\": MODEL_NAME,\n",
    "    \"artifact_uri\": MODEL_URI,\n",
    "    \"metadata_schema_uri\": \"\",\n",
    "    \"container_spec\": CONTAINER_SPEC,\n",
    "    \"explanation_spec\": EXPLANATION_SPEC,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVaVaTms5AFz"
   },
   "source": [
    "### Upload the model\n",
    "Sometimes it can take a while for permissions to propagate for example-based explanations. If you get an internal error, please retry once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long running operation: projects/791305858216/locations/us-central1/models/2849190869335015424/operations/4219303647420350464\n"
     ]
    }
   ],
   "source": [
    "def upload_model(model_configuration):\n",
    "\n",
    "    model = vertex_ai_v1beta1.Model(\n",
    "        display_name=model_configuration[\"display_name\"],\n",
    "        artifact_uri=model_configuration[\"artifact_uri\"],\n",
    "        metadata_schema_uri=model_configuration[\"metadata_schema_uri\"],\n",
    "        explanation_spec=model_configuration[\"explanation_spec\"],\n",
    "        container_spec=model_configuration[\"container_spec\"],\n",
    "    )\n",
    "\n",
    "    response = clients[\"model\"].upload_model(parent=PARENT, model=model)\n",
    "    print(\"Long running operation:\", response.operation.name)\n",
    "    upload_model_response = response.result()\n",
    "    return upload_model_response\n",
    "\n",
    "\n",
    "def get_model(name):\n",
    "    response = clients[\"model\"].get_model(name=name)\n",
    "    print(response)\n",
    "\n",
    "# def benchmark_upload_model(model_configuration, NUM_ITERATIONS):\n",
    "#     upload_time_taken = []\n",
    "#     for i in range(NUM_ITERATIONS):\n",
    "#         start = time.time()\n",
    "#         upload_model_response = upload_model(model_configuration)\n",
    "#         end = time.time()\n",
    "#         time_taken = end - start\n",
    "#         upload_time_taken.append(time_taken)\n",
    "#         print(f\"Time taken to upload the model: {time_taken:.5f} secs\")\n",
    "#     print(f\"Time taken to upload the model for {NUM_ITERATIONS} iterations: {upload_time_taken}\")\n",
    "#     return upload_time_taken, upload_model_response\n",
    "\n",
    "\n",
    "# start = time.time()\n",
    "# upload_model_response = upload_model(MODEL_CONFIGURATION)\n",
    "# end = time.time()\n",
    "# print(f\"Time taken to upload the model: {end - start:.5f} secs\")\n",
    "# uploaded_model_id = upload_model_response.model\n",
    "# uploaded_model_id = upload_model(MODEL_CONFIGURATION)\n",
    "\n",
    "\n",
    "# get_model(uploaded_model_id)\n",
    "upload_model_response = upload_model(model_configuration)\n",
    "# upload_time_taken, upload_model_response = benchmark_upload_model(MODEL_CONFIGURATION, NUM_ITERATIONS)\n",
    "uploaded_model_id = upload_model_response.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XR36Pe7Ij1S4"
   },
   "source": [
    "# Deploy the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApkeAPaeuDOw"
   },
   "source": [
    "## Create endpoint for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long running operation: projects/791305858216/locations/us-central1/endpoints/2742837308603498496/operations/5032973038300168192\n"
     ]
    }
   ],
   "source": [
    "ENDPOINT_NAME = f\"similarity-{DATASET_NAME}-endpoint-{TIMESTAMP}\"\n",
    "DESCRIPTION = \"An endpoint for the similarity model\"\n",
    "LABELS = {\"status\": \"online\"}\n",
    "\n",
    "\n",
    "def create_endpoint(display_name, description, labels):\n",
    "    endpoint = {\n",
    "        \"display_name\": display_name,\n",
    "        \"description\": description,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "    response = clients[\"endpoint\"].create_endpoint(parent=PARENT, endpoint=endpoint)\n",
    "    print(\"Long running operation:\", response.operation.name)\n",
    "\n",
    "    result = response.result()\n",
    "\n",
    "    return result\n",
    "PARENT = f'projects/{PROJECT_ID}/locations/{REGION}'\n",
    "endpoint_response = create_endpoint('new_text_news', DESCRIPTION, LABELS)\n",
    "endpoint_id = endpoint_response.name\n",
    "endpoint_short_id = endpoint_id.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tg1-_BaZuWFk"
   },
   "source": [
    "## Deploy the uploaded model to the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploy machine type n1-standard-4\n",
      "Long running operation: projects/791305858216/locations/us-central1/endpoints/2742837308603498496/operations/2956813610082369536\n",
      "Time taken to deploy model: 574.44398 secs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "DEPLOYED_NAME = f\"similarity-{DATASET_NAME}-deployed-{TIMESTAMP}\"\n",
    "\n",
    "if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (\n",
    "        vertex_ai_v1beta1.AcceleratorType.NVIDIA_TESLA_K80,\n",
    "        int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")),\n",
    "    )\n",
    "else:\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (None, None)\n",
    "    \n",
    "if os.getenv(\"IS_TESTING_DEPLOY_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_DEPLOY_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)\n",
    "    \n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "def deploy_model(\n",
    "    model, deployed_model_display_name, endpoint, traffic_split={\"0\": 100}\n",
    "):\n",
    "\n",
    "    if DEPLOY_GPU:\n",
    "        machine_spec = {\n",
    "            \"machine_type\": DEPLOY_COMPUTE,\n",
    "            \"accelerator_type\": DEPLOY_GPU,\n",
    "            \"accelerator_count\": DEPLOY_NGPU,\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\n",
    "            \"machine_type\": DEPLOY_COMPUTE,\n",
    "            \"accelerator_count\": 0,\n",
    "        }\n",
    "\n",
    "    deployed_model = {\n",
    "        \"model\": model,\n",
    "        \"display_name\": deployed_model_display_name,\n",
    "        \"dedicated_resources\": {\n",
    "            \"min_replica_count\": MIN_NODES,\n",
    "            \"max_replica_count\": MAX_NODES,\n",
    "            \"machine_spec\": machine_spec,\n",
    "        },\n",
    "        \"enable_container_logging\": False,\n",
    "    }\n",
    "\n",
    "    response = clients[\"endpoint\"].deploy_model(\n",
    "        endpoint=endpoint, deployed_model=deployed_model, traffic_split=traffic_split\n",
    "    )\n",
    "\n",
    "    print(\"Long running operation:\", response.operation.name)\n",
    "    result = response.result()\n",
    "#     print(\"result\")\n",
    "    deployed_model = result.deployed_model\n",
    "\n",
    "    return deployed_model.id\n",
    "\n",
    "# def benchmark_deploy_model(model, deployed_model_display_name, endpoint, NUM_ITERATIONS):\n",
    "#     time_taken_list = []\n",
    "#     for i in range(NUM_ITERATIONS):\n",
    "#         start = time.time()\n",
    "#         deployed_model_id = deploy_model(model, deployed_model_display_name, endpoint)\n",
    "#         end = time.time()\n",
    "#         time_taken = end - start\n",
    "#         time_taken_list.append(time_taken)\n",
    "#         print(f\"Time taken to deploy the model: {time_taken:.5f} secs\")\n",
    "#     print(f\"Time taken to deploy the model for {NUM_ITERATIONS} iterations: {time_taken_list}\")\n",
    "#     return time_taken_list, deployed_model_id\n",
    "\n",
    "# start = time.time()\n",
    "uploaded_model_id = result.model\n",
    "# upload_model_id = 'projects/'+PROJECT_ID+'/locations/'+REGION+'/models/'+uploaded_model_id\n",
    "deployed_model_id = deploy_model(upload_model_id, DEPLOYED_NAME, endpoint_id)\n",
    "# end = time.time()\n",
    "# print(f\"Time taken to deploy model: {end - start:.5f} secs\")\n",
    "# time_taken_list, deployed_model_id = benchmark_deploy_model(uploaded_model_id, DEPLOYED_NAME, endpoint_id, NUM_ITERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVgbmM1CkYH4"
   },
   "source": [
    "# Request example-based explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAFuNUq5yr5s"
   },
   "source": [
    "## Prepare the validation data\n",
    "We will issue queries for this data. For the purpuse of the demo, we will choose a small subset of the full validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "id": "-BgUO1dEzNvU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Example batch shape: 1\n",
      "Example batch shape: (512,)\n",
      "Time taken to process validation data: 0.03566 secs\n"
     ]
    }
   ],
   "source": [
    "input_tensor_name = 'input_text' \n",
    "val_dataset_file = f'{DATASET_NAME}-val-examples.jsonl'\n",
    "saved_val_jsonl_path = f'{PREPROCESSED_DIR}/{val_dataset_file}'\n",
    "\n",
    "num_batches = 1\n",
    "start = time.time()\n",
    "val_examples, val_labels = extract_examples_and_labels(validation_ds, num_batches=num_batches)\n",
    "end = time.time()\n",
    "print(f\"Time taken to process validation data: {end - start:.5f} secs\")\n",
    "val_ex=val_examples[:30]\n",
    "with open(saved_val_jsonl_path, 'w') as f:\n",
    "    for i,ex in enumerate(val_ex):\n",
    "        json.dump({'id':str(i), input_tensor_name:str(ex)}, f)\n",
    "        f.write('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "id": "qp81GP_b8Zsy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 data points.\n"
     ]
    }
   ],
   "source": [
    "val_data = []\n",
    "with open(saved_val_jsonl_path) as f:\n",
    "  for line in f:\n",
    "    val_data.append(json.loads(line))\n",
    "print(f'{len(val_data)} data points.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbGQYNgAk5Uu"
   },
   "source": [
    "## Run prediction requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "w9sVaLKBuY3Y"
   },
   "outputs": [],
   "source": [
    "# from google.cloud import aiplatform\n",
    "def explain_example(formatted_data, endpoint, parameters, deployed_model_id):\n",
    "\n",
    "    # The format of each instance should conform to the deployed model's prediction input schema.\n",
    "    instances_list = formatted_data\n",
    "    instances = [\n",
    "        json_format.ParseDict(instance, Value()) for instance in instances_list\n",
    "    ]\n",
    "    # endpoint = aiplatform.Endpoint(endpoint_id)\n",
    "    # response = endpoint.explain(instances=instances, parameters=parameters,deployed_model_id=deployed_model_id)\n",
    "    response = clients[\"prediction\"].explain(\n",
    "        endpoint=endpoint,\n",
    "        instances=instances,\n",
    "        parameters=parameters,\n",
    "        # deployed_model_id=deployed_model_id,\n",
    "    )\n",
    "    return response\n",
    "response = explain_example(val_data, endpoint_id, None, deployed_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "id": "QsjW5rA6Ejyb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to explain 8 examples: 0.26765 secs\n",
      "Time taken to explain 8 examples: 0.25737 secs\n",
      "Time taken to explain 8 examples: 0.20978 secs\n",
      "Time taken to explain 6 examples: 0.20255 secs\n",
      "\n",
      "Processed: 30 examples in batches of size 8. Iterations: 4\n",
      "Time taken to explain examples for 4 iterations: [0.26764535903930664, 0.2573704719543457, 0.2097795009613037, 0.20255184173583984]\n",
      "Examples processed in 4 iterations: [8, 8, 8, 6]\n",
      "Time taken to generate explanations: 0.94636 secs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_explanations(val_data, NUM_VAL_DATA, BATCH_SIZE, endpoint_id, deployed_model_id):\n",
    "    dataset_size = len(val_data)\n",
    "    all_neighbors = []\n",
    "    time_taken_list = []\n",
    "    examples_processed_each_iter = []\n",
    "    if NUM_VAL_DATA > dataset_size:\n",
    "        print(f\"Requesting {NUM_VAL_DATA} explanations while the dataset is only {dataset_size}\")\n",
    "    for data_idx in range(0, NUM_VAL_DATA, BATCH_SIZE):\n",
    "        end_idx = min(data_idx + BATCH_SIZE, NUM_VAL_DATA)\n",
    "        formatted_data = val_data[data_idx:end_idx]\n",
    "        start = time.time()\n",
    "        response = explain_example(formatted_data, endpoint_id, None, deployed_model_id)\n",
    "        end = time.time()\n",
    "        time_taken = end - start\n",
    "        time_taken_list.append(time_taken)\n",
    "        examples_processed_each_iter.append(len(formatted_data))\n",
    "        print(f\"Time taken to explain {len(formatted_data)} examples: {time_taken:.5f} secs\")\n",
    "        all_neighbors = (\n",
    "            all_neighbors + json_format.MessageToDict(response._pb)[\"explanations\"]\n",
    "        )\n",
    "    print(f\"\\nProcessed: {len(all_neighbors)} examples in batches of size {BATCH_SIZE}. Iterations: {len(time_taken_list)}\")\n",
    "    print(f\"Time taken to explain examples for {len(time_taken_list)} iterations: {time_taken_list}\")\n",
    "    print(f\"Examples processed in {len(time_taken_list)} iterations: {examples_processed_each_iter}\")\n",
    "    return all_neighbors, time_taken_list, examples_processed_each_iter\n",
    "\n",
    "BATCH_SIZE = (\n",
    "    8  # The request payload has a size limit so we need to subbatch our request\n",
    ")\n",
    "NUM_VAL_DATA = 30\n",
    "\n",
    "start = time.time()\n",
    "all_neighbors, time_taken_list, examples_processed_each_iter = get_explanations(val_data, NUM_VAL_DATA, BATCH_SIZE, endpoint_id, deployed_model_id)\n",
    "end = time.time()\n",
    "print(f\"Time taken to generate explanations: {end - start:.5f} secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "id": "MW9jGL1hwBXW"
   },
   "outputs": [],
   "source": [
    "# Save input ids and the corresponding neighbors\n",
    "data_with_neighbors = []\n",
    "input_data_list = val_data[:NUM_VAL_DATA]\n",
    "\n",
    "for i, input_data in enumerate(input_data_list):\n",
    "    neighbor_dict = all_neighbors[i]\n",
    "    neighbor_dict[\"input\"] = input_data[\"id\"]\n",
    "    data_with_neighbors.append(neighbor_dict)\n",
    "\n",
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    val_idx = 0\n",
    "    print(data_with_neighbors[val_idx])\n",
    "    print(data_with_neighbors[val_idx][\"neighbors\"])\n",
    "    print(data_with_neighbors[val_idx][\"input\"])\n",
    "    print(len(data_with_neighbors[val_idx][\"neighbors\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9Gv-HxLlDOq"
   },
   "source": [
    "## Inspect explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input label: World(0). Input index: 29. Input example:\n",
      "AFP - Microsoft said it will join with India's second-largest software firm, Infosys Technologies, to provide software and consulting to manufacturing, banking and automobile companies.\n",
      "\n",
      "Neighbor label: Sci/Tech(3). Neighbor index: 90937. Neighbor distance: 0.964. Neighbor example:\n",
      " AP - Software giant Microsoft Corp. signed software partnerships Monday with India's leading outsourcing firms, Infosys Technologies Ltd. and Wipro Ltd., and stepped up plans to hire more programmers\n",
      "in India.\n",
      "\n",
      "Neighbor label: Sci/Tech(3). Neighbor index: 100302. Neighbor distance: 1.158. Neighbor example:\n",
      "  BERKELEY, Calif. (Reuters) - The United States has nothing  to fear from rapidly growing technology markets in China and  India, Bill Gates, chairman and chief software architect of  Microsoft Corp.\n",
      "&lt;A HREF=\"http://www.reuters.co.uk/financeQuoteLookup.jhtml?ticker=MSFT.O qtype=sym infotype=info qcat=news\"&gt;MSFT.O&lt;/A&gt; said on Friday.\n",
      "\n",
      "Neighbor label: Sci/Tech(3). Neighbor index: 107298. Neighbor distance: 1.166. Neighbor example:\n",
      " AP - Boeing Co. and International Business Machines Corp. plan to develop advanced information technologies for the Defense Department and intelligence systems under a 10-year partnership the\n",
      "companies announced Monday.\n",
      "\n",
      "Neighbor label: Business(2). Neighbor index: 102381. Neighbor distance: 1.236. Neighbor example:\n",
      " LONDON -- The U.K.'s National Health Service (NHS) has tapped IT researcher Gartner Inc. to provide market intelligence services as the health organization forges ahead with a mammoth, 5 billion\n",
      "(\\$9.2 billion) project to upgrade its information technology infrastructure.\n",
      "\n",
      "Neighbor label: Sci/Tech(3). Neighbor index: 36815. Neighbor distance: 1.246. Neighbor example:\n",
      " Google has seen many benefits from its internal Web log system and may consider providing blogging tools and expertise to corporate clients, a company executive said.\n",
      "\n",
      "Neighbor label: Sci/Tech(3). Neighbor index: 19813. Neighbor distance: 1.269. Neighbor example:\n",
      " After human resources, finance and accounting and supply chain management, large corporations and government agencies are turning to outside contractors for security.\n",
      "\n",
      "Neighbor label: Business(2). Neighbor index: 17590. Neighbor distance: 1.393. Neighbor example:\n",
      " BERKELEY: The US has nothing to fear from rapidly growing technology markets in India and China, Bill Gates, chairman and chief software architect of Microsoft said.\n",
      "\n",
      "Neighbor label: Sci/Tech(3). Neighbor index: 104605. Neighbor distance: 1.450. Neighbor example:\n",
      " Dell (Dell:Nasdaq - news - research) and Microsoft (MSFT:Nasdaq - news - research) are partnering to develop and sell software that makes it easier for large businesses to manage their servers, the\n",
      "two companies announced Monday.\n",
      "\n",
      "Neighbor label: Sci/Tech(3). Neighbor index: 22771. Neighbor distance: 1.474. Neighbor example:\n",
      " CHICAGO - (BUSINESS WIRE) -ept. 14, 2004  -ay IBM announced a broad suite of services to speed the benefits of radio frequency identification (RFID) systems to industrial companies and mid-market\n",
      "businesses.\n",
      "\n",
      "Neighbor label: Sci/Tech(3). Neighbor index: 114332. Neighbor distance: 1.616. Neighbor example:\n",
      " PeopleSoft said it will work with IBM to optimize its apps for use with IBM's WebSphere middleware and development tools, and plans to begin selling WebSphere products directly through its own sales\n",
      "force.\n",
      "\n",
      "*****************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_example_indices = [29] # examples to inspect\n",
    "for val_example_idx in val_example_indices:\n",
    "    if val_example_idx > NUM_VAL_DATA - 1:\n",
    "        print(f'\\n\\n****Data index {val_example_idx} does not exist in the requested explanations***\\n\\n')\n",
    "        continue\n",
    "    inspect_input_and_neighbors(val_example_idx, all_train_examples, val_examples, all_train_labels, val_labels, label_index_to_name, data_with_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLmS2M6dWsAI"
   },
   "source": [
    "## Further exploration\n",
    "If you want to continue exploring, here are some ideas:\n",
    "1.   Isolate test points where the model is making mistakes (cat mislabed as bird), and visualize the example-based explanations to see if you can find any common patterns.\n",
    "2.   If through this analysis, you find your training data is lacking in some representative cases (overhead images of cats), you can try adding such images to your dataset to see if that improves model performance.\n",
    "3.   [Fine-tune](https://keras.io/guides/transfer_learning/) the lower layers of the model to see if you can improve the quality of example-based explanations by enabling the model to learn a better latent representation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3Hr2qMyfv2Q"
   },
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "endpoint_list = vertex_ai.Endpoint.list(filter=f'display_name=\"{ENDPOINT_NAME}\"')\n",
    "for e in endpoint_list:\n",
    "  e.delete(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnZh0DirATUP"
   },
   "outputs": [],
   "source": [
    "# Delete model\n",
    "model_list = vertex_ai.Model.list(filter=f'display_name=\"{MODEL_NAME}\"')\n",
    "for m in model_list:\n",
    "  m.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "# Delete Cloud Storage objects that were created\n",
    "delete_bucket = False\n",
    "\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -rf {BUCKET_URI}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Example-based Explanations Demo [Images] - v2.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
